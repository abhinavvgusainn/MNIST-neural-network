{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:54:23.260786Z","iopub.execute_input":"2025-12-23T08:54:23.261316Z","iopub.status.idle":"2025-12-23T08:54:23.265538Z","shell.execute_reply.started":"2025-12-23T08:54:23.261287Z","shell.execute_reply":"2025-12-23T08:54:23.264543Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:55:17.296268Z","iopub.execute_input":"2025-12-23T08:55:17.296563Z","iopub.status.idle":"2025-12-23T08:55:19.546985Z","shell.execute_reply.started":"2025-12-23T08:55:17.296541Z","shell.execute_reply":"2025-12-23T08:55:19.546129Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:57:17.081313Z","iopub.execute_input":"2025-12-23T08:57:17.081834Z","iopub.status.idle":"2025-12-23T08:57:17.095922Z","shell.execute_reply.started":"2025-12-23T08:57:17.081809Z","shell.execute_reply":"2025-12-23T08:57:17.095074Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"data = np.array(data)\nm, n = data.shape\nnp.random.shuffle(data)\n\ndata_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n]\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\n\n# Scaling pixels to a 0-1 range helps the gradients stay small and stable.\nX_train = X_train / 255.\nX_dev = X_dev / 255.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef init_params():\n    \"\"\"\n    INITIALIZATION\n    Generates weights (W) and biases (b) for a 2-layer network.\n    - Layer 1: 784 inputs (pixels) -> 10 hidden units.\n    - Layer 2: 10 hidden units -> 10 outputs (digits 0-9).\n    - Subtraction of 0.5 centers the initial weights around zero.\n    \"\"\"\n    W1 = np.random.randn(10, 784) * np.sqrt(2. / 784)\n    b1 = np.zeros((10, 1)) # Starting biases at 0 is standard\n    W2 = np.random.randn(10, 10) * np.sqrt(2. / 10)\n    b2 = np.zeros((10, 1))\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    \"\"\"\n    RECTIFIED LINEAR UNIT (Activation Function)\n    Returns Z if Z > 0, otherwise returns 0. \n    Introduces non-linearity, allowing the network to learn complex patterns.\n    \"\"\"\n    return np.maximum(0, Z)\n\ndef softmax(Z):\n    \"\"\"\n    SOFTMAX (Output Activation)\n    Converts raw scores (logits) into probabilities that sum to 1.\n    Uses 'Z - max(Z)' to prevent numerical overflow of the exp() function.\n    \"\"\"\n    exp = np.exp(Z - np.max(Z, axis=0))\n    return exp / np.sum(exp, axis=0)\n\ndef forward_prop(W1, b1, W2, b2, X):\n    \"\"\"\n    FORWARD PROPAGATION\n    Calculates the model's guess by passing inputs through the layers:\n    1. Z1: Weighted sum of inputs for Layer 1.\n    2. A1: Activation of Z1 using ReLU.\n    3. Z2: Weighted sum of A1 for Layer 2.\n    4. A2: Final probabilities using Softmax.\n    \"\"\"\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef one_hot(Y):\n    \"\"\"\n    ONE-HOT ENCODING\n    Converts integer labels (e.g., 3) into binary vectors (e.g., [0,0,0,1,0...]).\n    This is necessary to compare the labels directly against the 10 output nodes.\n    \"\"\"\n    one_hot_Y = np.zeros((Y.size, int(Y.max() + 1)))\n    one_hot_Y[np.arange(Y.size), Y.astype(int)] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef deriv_ReLU(Z):\n    \"\"\"\n    ReLU DERIVATIVE\n    Returns 1 for positive values and 0 for negative values.\n    Used during backpropagation to determine how much the ReLU slope \n    affects the final error.\n    \"\"\"\n    return Z > 0\n\ndef back_prop(Z1, A1, Z2, A2, W2, X, Y):\n    \"\"\"\n    BACKWARD PROPAGATION\n    Calculates gradients (slopes) using the chain rule to see how \n    parameters should change to reduce error:\n    - dZ2: The error at the output layer.\n    - dW/db: The sensitivity of the error to specific weights and biases.\n    - m: The number of training examples, used to average the gradients.\n    \"\"\"\n    m = Y.size\n    one_hot_Y = one_hot(Y)\n    \n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    \"\"\"\n    PARAMETER UPDATE (Optimization)\n    Adjusts weights and biases by subtracting a fraction (alpha) of the gradient.\n    - alpha: The learning rate.\n    - This 'nudges' the parameters toward a state where error is minimized.\n    \"\"\"\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1\n    W2 = W2 - alpha * dW2\n    b2 = b2 - alpha * db2\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:10:27.600504Z","iopub.execute_input":"2025-12-23T10:10:27.601348Z","iopub.status.idle":"2025-12-23T10:10:27.612965Z","shell.execute_reply.started":"2025-12-23T10:10:27.601320Z","shell.execute_reply":"2025-12-23T10:10:27.611936Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import numpy as np\n\ndef get_predictions(A2):\n    \"\"\"\n    PREDICTION EXTRACTION\n    Processes the Softmax output layer (A2) to find the model's final guess.\n    - A2 contains probabilities for each class (0-9) in columns.\n    - np.argmax finds the index of the highest probability value.\n    - Returns a 1D array of predicted digit labels.\n    \"\"\"\n    return np.argmax(A2, axis=0)\n\ndef get_accuracy(predictions, Y):\n    \"\"\"\n    ACCURACY METRIC\n    Calculates the performance of the model by comparing guesses to reality.\n    - (predictions == Y) creates a boolean array (True for correct, False for wrong).\n    - np.sum treats True as 1 and False as 0.\n    - Dividing by Y.size yields the percentage of correct classifications.\n    \"\"\"\n    acc = np.sum(predictions == Y) / Y.size\n    return acc\n\ndef gradient_descent(X, Y, iterations, alpha):\n    \"\"\"\n    THE TRAINING LOOP (Stochastic/Batch Gradient Descent)\n    The main engine that iterates through the learning process to optimize weights.\n    \n    1. INITIALIZATION: Starts with random weights and biases.\n    2. FORWARD PROP: Passes the input data through the network to generate an output.\n    3. BACKWARD PROP: Calculates the gradient (direction) to move to reduce error.\n    4. UPDATE: Steps the weights/biases in that direction by a factor of alpha.\n    5. LOGGING: Periodically prints accuracy to monitor the model's 'learning curve'.\n    \n    Returns the final optimized weights and biases (the 'trained' model).\n    \"\"\"\n    W1, b1, W2, b2 = init_params()\n    \n    for i in range(iterations):\n        # Pass data through the network layers\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        \n        # Calculate error gradients via the chain rule\n        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y)\n        \n        # Adjust parameters based on gradients and learning rate (alpha)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        \n        # Report progress every 10 iterations to observe convergence\n        if i % 10 == 0:\n            predictions = get_predictions(A2)\n            accuracy = get_accuracy(predictions, Y)\n            print(f\"Iteration: {i} | Accuracy: {accuracy:.4f}\")\n            \n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:10:31.501008Z","iopub.execute_input":"2025-12-23T10:10:31.501336Z","iopub.status.idle":"2025-12-23T10:10:31.508899Z","shell.execute_reply.started":"2025-12-23T10:10:31.501312Z","shell.execute_reply":"2025-12-23T10:10:31.508105Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 1500, 0.15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:10:34.640424Z","iopub.execute_input":"2025-12-23T10:10:34.641193Z","iopub.status.idle":"2025-12-23T10:12:28.786446Z","shell.execute_reply.started":"2025-12-23T10:10:34.641168Z","shell.execute_reply":"2025-12-23T10:12:28.785461Z"}},"outputs":[{"name":"stdout","text":"Iteration: 0 | Accuracy: 0.0792\nIteration: 10 | Accuracy: 0.3430\nIteration: 20 | Accuracy: 0.6110\nIteration: 30 | Accuracy: 0.7444\nIteration: 40 | Accuracy: 0.7947\nIteration: 50 | Accuracy: 0.8190\nIteration: 60 | Accuracy: 0.8341\nIteration: 70 | Accuracy: 0.8452\nIteration: 80 | Accuracy: 0.8527\nIteration: 90 | Accuracy: 0.8598\nIteration: 100 | Accuracy: 0.8648\nIteration: 110 | Accuracy: 0.8697\nIteration: 120 | Accuracy: 0.8733\nIteration: 130 | Accuracy: 0.8762\nIteration: 140 | Accuracy: 0.8787\nIteration: 150 | Accuracy: 0.8819\nIteration: 160 | Accuracy: 0.8837\nIteration: 170 | Accuracy: 0.8858\nIteration: 180 | Accuracy: 0.8878\nIteration: 190 | Accuracy: 0.8895\nIteration: 200 | Accuracy: 0.8914\nIteration: 210 | Accuracy: 0.8930\nIteration: 220 | Accuracy: 0.8941\nIteration: 230 | Accuracy: 0.8953\nIteration: 240 | Accuracy: 0.8964\nIteration: 250 | Accuracy: 0.8976\nIteration: 260 | Accuracy: 0.8988\nIteration: 270 | Accuracy: 0.8998\nIteration: 280 | Accuracy: 0.9007\nIteration: 290 | Accuracy: 0.9017\nIteration: 300 | Accuracy: 0.9027\nIteration: 310 | Accuracy: 0.9038\nIteration: 320 | Accuracy: 0.9047\nIteration: 330 | Accuracy: 0.9052\nIteration: 340 | Accuracy: 0.9059\nIteration: 350 | Accuracy: 0.9069\nIteration: 360 | Accuracy: 0.9074\nIteration: 370 | Accuracy: 0.9079\nIteration: 380 | Accuracy: 0.9082\nIteration: 390 | Accuracy: 0.9087\nIteration: 400 | Accuracy: 0.9094\nIteration: 410 | Accuracy: 0.9102\nIteration: 420 | Accuracy: 0.9109\nIteration: 430 | Accuracy: 0.9114\nIteration: 440 | Accuracy: 0.9115\nIteration: 450 | Accuracy: 0.9120\nIteration: 460 | Accuracy: 0.9125\nIteration: 470 | Accuracy: 0.9130\nIteration: 480 | Accuracy: 0.9135\nIteration: 490 | Accuracy: 0.9138\nIteration: 500 | Accuracy: 0.9142\nIteration: 510 | Accuracy: 0.9143\nIteration: 520 | Accuracy: 0.9145\nIteration: 530 | Accuracy: 0.9147\nIteration: 540 | Accuracy: 0.9150\nIteration: 550 | Accuracy: 0.9152\nIteration: 560 | Accuracy: 0.9156\nIteration: 570 | Accuracy: 0.9158\nIteration: 580 | Accuracy: 0.9161\nIteration: 590 | Accuracy: 0.9164\nIteration: 600 | Accuracy: 0.9166\nIteration: 610 | Accuracy: 0.9168\nIteration: 620 | Accuracy: 0.9170\nIteration: 630 | Accuracy: 0.9172\nIteration: 640 | Accuracy: 0.9174\nIteration: 650 | Accuracy: 0.9176\nIteration: 660 | Accuracy: 0.9178\nIteration: 670 | Accuracy: 0.9181\nIteration: 680 | Accuracy: 0.9183\nIteration: 690 | Accuracy: 0.9186\nIteration: 700 | Accuracy: 0.9188\nIteration: 710 | Accuracy: 0.9190\nIteration: 720 | Accuracy: 0.9190\nIteration: 730 | Accuracy: 0.9192\nIteration: 740 | Accuracy: 0.9193\nIteration: 750 | Accuracy: 0.9195\nIteration: 760 | Accuracy: 0.9198\nIteration: 770 | Accuracy: 0.9201\nIteration: 780 | Accuracy: 0.9202\nIteration: 790 | Accuracy: 0.9205\nIteration: 800 | Accuracy: 0.9208\nIteration: 810 | Accuracy: 0.9210\nIteration: 820 | Accuracy: 0.9212\nIteration: 830 | Accuracy: 0.9213\nIteration: 840 | Accuracy: 0.9214\nIteration: 850 | Accuracy: 0.9215\nIteration: 860 | Accuracy: 0.9216\nIteration: 870 | Accuracy: 0.9217\nIteration: 880 | Accuracy: 0.9218\nIteration: 890 | Accuracy: 0.9219\nIteration: 900 | Accuracy: 0.9221\nIteration: 910 | Accuracy: 0.9222\nIteration: 920 | Accuracy: 0.9223\nIteration: 930 | Accuracy: 0.9225\nIteration: 940 | Accuracy: 0.9226\nIteration: 950 | Accuracy: 0.9228\nIteration: 960 | Accuracy: 0.9228\nIteration: 970 | Accuracy: 0.9230\nIteration: 980 | Accuracy: 0.9231\nIteration: 990 | Accuracy: 0.9232\nIteration: 1000 | Accuracy: 0.9233\nIteration: 1010 | Accuracy: 0.9235\nIteration: 1020 | Accuracy: 0.9236\nIteration: 1030 | Accuracy: 0.9237\nIteration: 1040 | Accuracy: 0.9238\nIteration: 1050 | Accuracy: 0.9240\nIteration: 1060 | Accuracy: 0.9241\nIteration: 1070 | Accuracy: 0.9242\nIteration: 1080 | Accuracy: 0.9244\nIteration: 1090 | Accuracy: 0.9244\nIteration: 1100 | Accuracy: 0.9245\nIteration: 1110 | Accuracy: 0.9245\nIteration: 1120 | Accuracy: 0.9246\nIteration: 1130 | Accuracy: 0.9247\nIteration: 1140 | Accuracy: 0.9249\nIteration: 1150 | Accuracy: 0.9250\nIteration: 1160 | Accuracy: 0.9251\nIteration: 1170 | Accuracy: 0.9253\nIteration: 1180 | Accuracy: 0.9253\nIteration: 1190 | Accuracy: 0.9254\nIteration: 1200 | Accuracy: 0.9254\nIteration: 1210 | Accuracy: 0.9255\nIteration: 1220 | Accuracy: 0.9256\nIteration: 1230 | Accuracy: 0.9257\nIteration: 1240 | Accuracy: 0.9258\nIteration: 1250 | Accuracy: 0.9259\nIteration: 1260 | Accuracy: 0.9261\nIteration: 1270 | Accuracy: 0.9263\nIteration: 1280 | Accuracy: 0.9263\nIteration: 1290 | Accuracy: 0.9265\nIteration: 1300 | Accuracy: 0.9266\nIteration: 1310 | Accuracy: 0.9267\nIteration: 1320 | Accuracy: 0.9268\nIteration: 1330 | Accuracy: 0.9268\nIteration: 1340 | Accuracy: 0.9268\nIteration: 1350 | Accuracy: 0.9269\nIteration: 1360 | Accuracy: 0.9270\nIteration: 1370 | Accuracy: 0.9271\nIteration: 1380 | Accuracy: 0.9272\nIteration: 1390 | Accuracy: 0.9274\nIteration: 1400 | Accuracy: 0.9275\nIteration: 1410 | Accuracy: 0.9276\nIteration: 1420 | Accuracy: 0.9277\nIteration: 1430 | Accuracy: 0.9277\nIteration: 1440 | Accuracy: 0.9278\nIteration: 1450 | Accuracy: 0.9279\nIteration: 1460 | Accuracy: 0.9280\nIteration: 1470 | Accuracy: 0.9281\nIteration: 1480 | Accuracy: 0.9281\nIteration: 1490 | Accuracy: 0.9283\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef make_predictions(X, W1, b1, W2, b2):\n    \"\"\"\n    INFERENCE ENGINE\n    Computes predictions for a given set of inputs using trained parameters.\n    - Unlike training, this only performs a Forward Pass.\n    - No backpropagation or parameter updates occur here.\n    - Returns the predicted class labels for the input matrix X.\n    \"\"\"\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions\n\ndef test_prediction(index, W1, b1, W2, b2, X_train, Y_train):\n    \"\"\"\n    VISUAL VALIDATION\n    Picks a specific example from the dataset, predicts the label, \n    and displays the image alongside the result.\n    - index: The location of the image in the dataset.\n    - The image is reshaped from a flat 784 array back to 28x28 for display.\n    \"\"\"\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n    label = Y_train[index]\n    \n    print(f\"Prediction: {prediction}\")\n    print(f\"Label: {label}\")\n    \n    # Reshape and display the image\n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()\n\ndef evaluate_dev_set(X_dev, Y_dev, W1, b1, W2, b2):\n    \"\"\"\n    FINAL EVALUATION\n    Runs the trained model on the 'Dev Set' (unseen data).\n    - This is the true test of the model's intelligence.\n    - If Dev Accuracy is much lower than Training Accuracy, the model has 'overfit' \n      (memorized the training data rather than learning patterns).\n    \"\"\"\n    dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n    accuracy = get_accuracy(dev_predictions, Y_dev)\n    print(f\"Final Dev Set Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:12:30.991183Z","iopub.execute_input":"2025-12-23T10:12:30.991505Z","iopub.status.idle":"2025-12-23T10:12:30.998941Z","shell.execute_reply.started":"2025-12-23T10:12:30.991481Z","shell.execute_reply":"2025-12-23T10:12:30.998125Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"evaluate_dev_set(X_dev, Y_dev, W1, b1, W2, b2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:12:34.050557Z","iopub.execute_input":"2025-12-23T10:12:34.050853Z","iopub.status.idle":"2025-12-23T10:12:34.057031Z","shell.execute_reply.started":"2025-12-23T10:12:34.050831Z","shell.execute_reply":"2025-12-23T10:12:34.056241Z"}},"outputs":[{"name":"stdout","text":"Final Dev Set Accuracy: 0.9090\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"test_prediction(7, W1, b1, W2, b2, X_train, Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:15:09.201791Z","iopub.execute_input":"2025-12-23T10:15:09.202112Z","iopub.status.idle":"2025-12-23T10:15:09.335892Z","shell.execute_reply.started":"2025-12-23T10:15:09.202089Z","shell.execute_reply":"2025-12-23T10:15:09.335114Z"}},"outputs":[{"name":"stdout","text":"Prediction: [7]\nLabel: 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":71}]}